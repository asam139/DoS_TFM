#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass ociamthesis-lyx
\begin_preamble
%Packages
\usepackage{listings}
\end_preamble
\options a4paper,titlepage
\use_default_options false
\maintain_unincluded_children false
\language spanish
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing onehalf
\use_hyperref false
\papersize a4paper
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type numerical
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 2
\paperpagestyle headings
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
Metodología
\begin_inset CommandInset label
LatexCommand label
name "chap:Metodología"

\end_inset


\end_layout

\begin_layout Standard
En esta sección se explicara detalladamente cada uno de los clasificadores
 objetos de estudio.
 Se comentará la procedencia de cada uno de ellos, ya sea estadística o
 de la IA, además se explicarán matemáticamente los fundamentos de cada
 uno de ellos.
\end_layout

\begin_layout Section
Modelos
\end_layout

\begin_layout Standard
En esta sección describiremos los diferentes tipos de modelos de clasificación
 que se utilizarán sobre la base de datos para su posterior comparación
 y validación.
 Se van a utilizar una serie de arboles de decisión estándares como son
 el C4.5, Naïve Bayes, LogR, SVM y Perceptrón multicapa.
 Y además se va a incorporar uno nuevo, el Credal Decision Trees (CDT),
 para ver como trabaja para la detección de ataques DDoS.
 
\end_layout

\begin_layout Subsection
Algoritmo C4.5
\end_layout

\begin_layout Standard
Es un algoritmo usado para genera un árbol de decisión desarrollado por
 Ross Quinlan 
\begin_inset CommandInset citation
LatexCommand citep
key "ProgramsForMachineLearning"

\end_inset

.
 Se trata de una extensión del algoritmo ID3 anteriormente por Quinlan.
 Aunque los arboles de decisión que este algoritmo genera tienen múltiples
 aplicaciones una de las más importantes es como clasificador.
 Por ello es muy común referirse a él como clasificador estadístico.
 El J48 es una implementación de código abierto en el lenguaje de programación
 Java.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Los algoritmos como el ID3 y el C4.5 que pertenecen a la familia Top Down
 Induction Trees (TDIDT
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "TDIDT"
description "Top Down Induction Trees"

\end_inset

), se construyen a partir del método de Hunt.
 El esqueleto de este método para construir un árbol de decisión a partir
 de un conjunto 
\begin_inset Formula $T$
\end_inset

 de datos de entrenamiento es muy simple.
 Sean las clases 
\begin_inset Formula ${C_{1},C_{2},C_{3},...,C_{k}}$
\end_inset

.
 Este algoritmo tiene unos pocos casos base.
\end_layout

\begin_layout Itemize
\begin_inset Formula $T$
\end_inset

 contiene uno o más casos, todos pertenecientes a una única clase 
\begin_inset Formula $C_{j}$
\end_inset

: el árbol de decisión para 
\begin_inset Formula $T$
\end_inset

 es una hoja identificando la clase 
\begin_inset Formula $C_{j}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
\begin_inset Formula $T$
\end_inset

 no contiene ningún caso: el árbol de decisión es una hoja, pero la clase
 asociada debe ser determinada por información que no pertenece a 
\begin_inset Formula $T.$
\end_inset

 Por ejemplo, una hoja puede escogerse de acuerdo a conocimientos de base
 del dominio, como ser la clase mayoritaria.
\end_layout

\begin_layout Itemize
\begin_inset Formula $T$
\end_inset

 contiene casos pertenecientes a varias clases: en este caso, la idea es
 refinar 
\begin_inset Formula $T$
\end_inset

 en subconjuntos de casos que tiendan, o parezcan tender hacia una colección
 de casos pertenecientes a una única clase.
 Se elige una prueba basada en un único.
\end_layout

\begin_layout Standard
C4.5 construye árboles de decisión desde un grupo de datos de entrenamiento
 de la misma forma en que lo hace ID3, usando el concepto de entropía de
 información 
\begin_inset CommandInset citation
LatexCommand citep
key "ProgramsForMachineLearning"

\end_inset

.
 La entropía mide la incertidumbre de una fuente de información, o también
 la cantidad de información que contienen los datos usados 
\begin_inset CommandInset citation
LatexCommand citep
key "Teoradelainformacincodificacinylenguajes"

\end_inset

.
 La entropía suponiendo que un evento (variable aleatoria) tiene un grado
 de indeterminación inicial igual a 
\begin_inset Formula $k$
\end_inset

 (es decir existen 
\begin_inset Formula $k$
\end_inset

 estados posibles) y supongamos todos los estados equiprobables.
 Entonces la probabilidad de que se dé una de esas combinaciones será 
\begin_inset Formula $p=\frac{1}{k}$
\end_inset

.
 Luego podemos representar la expresión 
\begin_inset Formula ${\displaystyle c_{i}}$
\end_inset

 como:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
{\displaystyle c_{i}=\log_{2}(k)=\log_{2}[1/(1/k)]=\log_{2}(1/p)=\underbrace{\log_{2}(1)}_{=0}-\log_{2}(p)=-\log_{2}(p)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Si ahora cada uno de los 
\begin_inset Formula $k$
\end_inset

 estados tiene una probabilidad 
\begin_inset Formula ${\displaystyle p_{i}}$
\end_inset

, entonces la entropía vendrá dada por la suma ponderada de la cantidad
 de información 
\begin_inset CommandInset citation
LatexCommand citep
key "Teoradelainformacincodificacinylenguajes"

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
H=-p_{1}\log_{2}(p_{1})-p_{2}\log_{2}(p_{2})-....-p_{k}\log_{2}(p_{k})=-\sum_{i=1}^{k}p_{i}\log_{2}(p_{i})
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Por lo tanto, la entropía de un mensaje 
\begin_inset Formula ${\displaystyle X}$
\end_inset

, denotado por 
\begin_inset Formula ${\displaystyle H(X)}$
\end_inset

, es el valor medio ponderado de la cantidad de información de los diversos
 estados del mensaje 
\begin_inset CommandInset citation
LatexCommand citep
key "Teoradelainformacincodificacinylenguajes"

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
{\displaystyle H(X)=-\sum_{i}p(x_{i})\log_{2}p(x_{i})}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
que representa una medida de la incertidumbre media acerca de una variable
 aleatoria y por tanto de la cantidad de información.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
En ID3 y posteriormente en C4.5, Quinlan hace una representación intuitiva
 de los árboles de clasificación, donde cada nodo se puede ver como una
 fuente de información de memoria nula, cuyo alfabeto tiene tantos símbolos
 como clases.
 Cada nodo al tener almacenado un subcojunto de casos del conjunto de datos,
 guarda una información media sobre las clases de esos casos.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
En ID3, el mejor atributo X para ramificar el árbol dado el conjunto de
 datos D, es aquel qeu maximiza la ganancia de información.
 Para calcular la ganancia de cada atributo se realizan los siguientes pasos:
\end_layout

\begin_layout Itemize
Se calcula la entropía de la clase C antes de ramificar, o lo que viene
 a ser lo mismo, la entropía de C para el conjunto D:
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\[
H_{D}(C)=-\sum_{i=1}^{k}p_{i}\log_{2}(p_{i})
\]

\end_inset

 donde 
\begin_inset Formula $p_{i}$
\end_inset

 representa la probabilidad (calculada por frecuencias relativas) de la
 clase 
\begin_inset Formula $i$
\end_inset

 en D y 
\begin_inset Formula $n$
\end_inset

es el número de clases.
\end_layout

\begin_layout Itemize
Se calcula el valor medio de la entropía en el nivel siguiente, generado
 por el atributo 
\begin_inset Formula $X$
\end_inset

:
\begin_inset Formula 
\[
H_{D}(C|X)=\sum_{i=1}^{k}p_{D}(X=x_{j})H_{D_{j}}(C)
\]

\end_inset

 donde 
\begin_inset Formula $\{x_{1,}x_{2,}...,x_{k}\}$
\end_inset

 son los valores del atributo, 
\begin_inset Formula $X,p_{d}(X=x_{j})$
\end_inset

representa la probabilidad condicionada de que 
\begin_inset Formula $X=x_{j}$
\end_inset

en 
\begin_inset Formula $D$
\end_inset

 y 
\begin_inset Formula $D_{j}$
\end_inset

 es el subconjunto de 
\begin_inset Formula $D,D_{j}\subset D$
\end_inset

, tal que sus elementos tienen el valor de 
\begin_inset Formula $X=x_{j}$
\end_inset

.
\end_layout

\begin_layout Itemize
Se denomina ganancia de la información de la variable X, a la diferencia
 de entropía de la entropía de C para el conjunto D menos la entropía media
 del siguiente nivel si ramificásemos por 
\begin_inset Formula $X$
\end_inset

:
\begin_inset Formula 
\[
Ganancia_{D}(C|X)=H_{D}(C)-H_{D}(C|X)
\]

\end_inset


\end_layout

\begin_layout Standard
En ID3, se escoge la variable con mayor ganancia de información.
 En C4.5 además se calcula la Información de ruptura, y la razón de ganancia.
 Para un conjunto de datos 
\begin_inset Formula $D$
\end_inset

 y un atributo 
\begin_inset Formula $X$
\end_inset

, se define la razón de ganancia de la siguiente forma:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
RazónGanancia(X|C)=\frac{Ganancia_{D}(C|X)}{InfoRuptura_{D}(X)}
\]

\end_inset


\end_layout

\begin_layout Standard
donde,
\begin_inset Formula 
\[
InfoRuptura_{D}(X)=-\sum_{j=1}^{k}p_{d}(X=x_{j})\log_{2}(p_{D}(X=x_{j}))
\]

\end_inset


\end_layout

\begin_layout Standard
La prueba basada en el criterio de máxima ganancia, tiende a escoger aquellas
 variables con mayor número de valores.
 Esto es debido a que cuantas más particiones se hagan debido a los valores
 de la variable, la entropía de un nuevo nodo será, normalmente menor.
 En C4.5 se utilza la razón de ganancia para paliar esta tendencia.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
El algoritmo C4.5 también se diferencia de ID3 porque admite datos continuos,
 dividiendo el dominio de la variable continua en subintervalos por el valor
 que tiene una mayor ganancia de información.
 Además C4.5 admite valores desconocidos en los ejemplos, modificando los
 criterios de ganancia e información de ruptura.
\end_layout

\begin_layout Subsection
Naïve Bayes
\end_layout

\begin_layout Standard
El clasificador Naïve Bayes es un simple clasificador probabilístico basado
 en la aplicación del teorema de Bayes con suposiciones de independencia
 fuertes (ingenuas) entre las características 
\begin_inset CommandInset citation
LatexCommand citep
key "ModelosBayesianosparalaclasificacinsupervisada.Aplicacionesalanlisisdedatosdeexpresingrfica"

\end_inset

.
 Naïve Bayes ha sido muy estudiado desde los años 50.
 Durante todos estos años sus aplicaciones han sido múltiples, una de ellas
 es como un método popular para la categorización de textos, es decir si
 son de política, deportes, anuncios, etc.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Naïve Bayes un algoritmo basado en un principio común: el valor de una caracterí
stica particular es independiente del valor de cualquier otra característica
 dada la variable de clase.
 Por ejemplo, una fruta puede ser considerada como una manzana si es roja,
 redonda y de unos 10 cm de diámetro.
 Un clasificador Naïve Bayes considera que cada una de estas características
 contribuye independientemente a la probabilidad de que este fruto sea una
 manzana, independientemente de las posibles correlaciones entre el color,
 la redondez y las características del diámetro.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Para algunos tipos de modelos de probabilidad, los clasificadores ingenuos
 de Bayes pueden ser entrenados de manera muy eficiente en un entorno de
 aprendizaje supervisado .
 En muchas aplicaciones prácticas, la estimación de parámetros para modelos
 ingenuos de Bayes utiliza el método de máxima verosimilitud; En otras palabras,
 se puede trabajar con él sin aceptar la probabilidad bayesiana o usar cualquier
 método bayesiano.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
A pesar de su diseño ingenuo y supuestos aparentemente simplificados, los
 clasificadores Naïve Bayes han funcionado bastante bien en muchas situaciones
 complejas del mundo real.
 En 2004, un análisis 
\begin_inset CommandInset citation
LatexCommand citep
key "TheOptimallyofNaiveBayes"

\end_inset

 del problema de la clasificación bayesiana mostró que existen razones teóricas
 sólidas para la aparentemente improbable eficacia de los clasificadores
 naif de Bayes.
 Sin embargo, una comparación completa con otros algoritmos de clasificación
 en 2006 
\begin_inset CommandInset citation
LatexCommand citep
key "Unacomparacinempricadealgoritmosdeaprendizajesupervisado"

\end_inset

mostró que la clasificación de Bayes es superada por otros enfoques, tales
 como los booted trees o los random forests.
\end_layout

\begin_layout Standard
Una ventaja de Naïve Bayes es que sólo requiere un pequeño número de datos
 de entrenamiento para estimar los parámetros necesarios para la clasificación.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Abstractamente, Naïve Bayes es un modelo de probabilidad condicional: dado
 un conjunto de datos a ser clasificados, representado por un vector 
\begin_inset Formula $x=(x_{1,}x_{2,}...,x_{n}$
\end_inset

 representando algunas características o atributos, 
\begin_inset Formula $n$
\end_inset

 (variables independientes), asigna a esta instancia las probabilidades
 
\begin_inset Formula $p(C_{k}|x_{1},...,x_{n}$
\end_inset

 para cada uno de los 
\begin_inset Formula $K$
\end_inset

 posibles resultados o clases 
\begin_inset Formula $C_{k}$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
key "PatternRecognition:AnAlgorithmicApproach."

\end_inset

.
 
\end_layout

\begin_layout Standard
El problema con la formulación anterior es que si el número de características
 
\begin_inset Formula $n$
\end_inset

 es grande o si una característica puede asumir un gran número de valores,
 entonces basar dicho modelo en tablas de probabilidad es inviable.
 Por lo tanto, se puede reformular el modelo para que sea más manejable.
 Usando el teorema de Bayes , la probabilidad condicional se puede descomponer
 como
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
p(C_{k}|x)=\frac{p(C_{k})p(x|C_{k})}{p(x)})\longrightarrow posterior=\frac{anterior*probabilidad}{evidencia}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
En la práctica, sólo hay interés en el numerador de esa fracción, porque
 el denominador no depende de 
\begin_inset Formula $C$
\end_inset

.
 Y los valores de las características 
\begin_inset Formula $F_{i}$
\end_inset

, de modo que el denominador es efectivamente constante.
 El numerador es equivalente a una probabilidad compuesta: 
\begin_inset Formula $p(C_{k},x_{1},...,x_{n})$
\end_inset

, que puede ser reescrita como sigue, aplicando repetidamente la definición
 de probabilidad condicional:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
p(C,F1,...,F_{n}=p(C)p(F_{1},...,F_{n}|C)=p(C)p(F_{1}|C)p(F_{2},...F_{n}|C,F_{1})=p(C)p(F_{1}|C)p(F_{2}|C,F_{1})p(F_{3},...F_{n}|C,F_{1},F_{2})
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
y así sucesivamente.
 Ahora es cuando el principio común de 
\begin_inset Quotes eld
\end_inset

naïve
\begin_inset Quotes erd
\end_inset

 de independencia condicional se aplica: se asume que cada 
\begin_inset Formula $F_{i}$
\end_inset

es independiente de cualquier otra 
\begin_inset Formula $F_{j}$
\end_inset

 para 
\begin_inset Formula $j\text{≠}i$
\end_inset

.
 Esto significa que 
\begin_inset Formula $p(F_{i}|C,F_{j})=p(F_{i}|C)$
\end_inset

, y por lo tanto que la probabilidad compuesta puede expresarse como:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
p(C,F_{1},...,F_{n})=p(C)p(F_{1}|C)p(F_{2}|C)p(F_{3}|C)...=p(C)\prod_{i=1}^{n}p(F_{i}|C)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Esto significa que haciendo estas asunciones, la distribución condicional
 sobre la variable clasificaroria {
\backslash
displaystyle C} C puede expresarse de la siguiente manera 
\begin_inset CommandInset citation
LatexCommand citep
key "PatternRecognition:AnAlgorithmicApproach."

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
p(C|F_{1},...,F_{n})=\frac{1}{Z}p(C)\prod_{i=1}^{n}p(F_{i}|C)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
donde 
\begin_inset Formula $Z$
\end_inset

 es un factor que depende sólo de 
\begin_inset Formula $F_{1},...,F_{n}$
\end_inset

, es decir, constante si los valores de 
\begin_inset Formula $F_{i}$
\end_inset

 son conocidos.
\begin_inset Newline newline
\end_inset

Hasta ahora se ha desarrollado el modelo de probabilidad de Bayes.
 Pero el clasificador combina este modelo junto con la regla de decisión.
 Una regla común es escoger la hípotesís mas probable; esto se conoce como
 el máximo a posteriori o regla de decisón MAP (
\shape italic
Maximum a Posteriori Probability
\shape default
)
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "MAP"
description "Maximum a Posteriori Probability"

\end_inset

.
 Por lo tanto el clasificador de Bayes es la función que asigna una etiqueta
 de clase 
\begin_inset Formula $\hat{y}=C_{k}$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
key "PatternRecognition:AnAlgorithmicApproach."

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{y}=\underset{k\epsilon\{1,...,K\}}{argmax}p(C_{k})\prod_{i=1}^{n}p(F_{i}|C)
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
LogR: Logictic Regression
\end_layout

\begin_layout Standard
La regresión logística o LogR
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "LogR"
description "Logictic Regression"

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
key "ModelosEstadsticos:TeorayPrctica"

\end_inset

 es un tipo de análisis de regresión utilizado para predecir el resultado
 de una variable que se puede categorizar, es decir, una variable qu epuede
 adoptar un número limitado de categorías en función de variables, característic
as o atributos.
 Es útil para modelar la probabilidad de un evento ocurriendo como función
 de otros factores.
 El análisis LogR se enmarca en el conjunto de Ge (GLM por sus siglas en
 inglés) que usa como función de enlace la función logit.
 Las probabilidades que describen el posible resultado de un único ensayo
 se modelan, como una función de variables explicativas, utilizando una
 función logística.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
La variable solo puede tener dos valores, es decir sólo dos valores 
\begin_inset Quotes eld
\end_inset

0
\begin_inset Quotes erd
\end_inset

 y 
\begin_inset Quotes eld
\end_inset

1
\begin_inset Quotes erd
\end_inset

, se denomina regresión lógistica binaria, la cual se explicará.
 En cambio si puede tener más ya se de habla de regresión lógistica multinomial.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
La regresión logística fue desarrollada por el estadístico David Cox en
 1958.
 
\begin_inset CommandInset citation
LatexCommand citep
key "Estimationoftheprobabilityofaneventasafunctionofseveralindependentvariables"

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
key "Theregressionanalysisofbinarysequenceswithdiscussion"

\end_inset

 El modelo logístico binario se utiliza para estimar la probabilidad de
 una respuesta binaria basada en una o más variables predictor (o independientes
) (características).
 Permite decir que la presencia de un factor de riesgo aumenta la probabilidad
 de un determinado resultado por un porcentaje específico.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
La regresión logística se utiliza en varios campos, incluyendo el aprendizaje
 automático, la mayoría de los campos médicos y las ciencias sociales.
 Por ejemplo, la regresión logística puede usarse para predecir si un paciente
 tiene una enfermedad dada (por ejemplo, diabetes , enfermedad coronaria),
 basándose en las características observadas del paciente (edad, sexo, índice
 de masa corporal 
\begin_inset CommandInset citation
LatexCommand citep
key "ValidationofMPIandOIAIIintwodifferentgroupsofpatientswithsecondaryperitonitis"

\end_inset

 , resultados de varios análisis de sangre , etc.).
 Ser utilizados en ingeniería , especialmente para predecir la probabilidad
 de fallo de un proceso, sistema o producto dado.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Una explicación de la regresión logística puede comenzar con una explicación
 de la función logística estándar.
 La función logística es útil porque puede tomar cualquier entrada real
 
\begin_inset Formula $t$
\end_inset

, 
\begin_inset Formula $(t\epsilon R)$
\end_inset

 , mientras que la salida siempre toma valores entre cero y uno 
\begin_inset CommandInset citation
LatexCommand citep
key "AppliedLogisticRegression"

\end_inset

 y por lo tanto es interpretable como una probabilidad.
 La función logística 
\begin_inset Formula $\sigma(t)$
\end_inset

 se define como:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\sigma(t)=\frac{e^{t}}{e^{t}+1}=\frac{1}{1+e^{-t}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Suponiendo que 
\begin_inset Formula $t$
\end_inset

 es una función lineal de una sola variable 
\begin_inset Formula $x$
\end_inset

, entonces se puede expresar 
\begin_inset Formula $t$
\end_inset

 como:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
t=\beta_{0}+\beta_{1}x
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Y la función lógistica ahora se puede escribir como:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
F(x)=\frac{1}{1+e^{-(\beta_{0}+\beta_{1}x)}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $F(x)$
\end_inset

 se interpreta como la probabilidad de que la variable dependiente sea igual
 a 
\begin_inset Quotes eld
\end_inset

éxito
\begin_inset Quotes erd
\end_inset

 o 
\begin_inset Quotes eld
\end_inset

caso
\begin_inset Quotes erd
\end_inset

, más que fallo o sin éxito.
 Y haciendo uso de F(x) se define la inversa de la función logística, logit,
 como:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
g(F(x))=ln\left(\frac{F(x)}{1-F(x)}\right)=\beta_{0}+\beta_{1}x\longrightarrow\frac{F(x)}{1-F(x)}=e^{\beta_{0}+\beta_{1}x}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Las probabilidades de que la variable dependiente sea igual a un caso (dada
 alguna combinación lineal 
\begin_inset Formula $x$
\end_inset

 de las variables predictor) es equivalente a la función exponencial de
 la expresión de regresión lineal.
 Esto ilustra cómo el logit sirve como una función de enlace entre la probabilid
ad y la expresión de regresión lineal.
 Dado que el logit oscila entre infinito negativo y positivo, proporciona
 un criterio adecuado sobre el cual conducir la regresión lineal y el logit
 se convierte fácilmente de nuevo en las probabilidades 
\begin_inset CommandInset citation
LatexCommand citep
key "AppliedLogisticRegression"

\end_inset

.
 
\end_layout

\begin_layout Subsection
SVM: Support Vector Machine
\end_layout

\begin_layout Standard
Las máquinas de soporte vectorial 
\begin_inset CommandInset citation
LatexCommand citep
key "AnIntroductiontoSupportVectorMachinesandotherkernel-basedlearningmethods"

\end_inset

, máquinas de vectores de soporte o máquinas de vector soporte (Support
 Vector Machines, SVMs) son un conjunto de algoritmos de aprendizaje supervisado
 desarrollados por Vladimir N.
 Vapnik y Alexey Ya.
 Chervonenkis en 1963.
 En 1992, Bernhard E.
 Boser , Isabelle M.
 Guyon y Vladimir N.
 Vapnik sugirieron una manera de crear clasificadores no lineales aplicando
 el truco del núcleo a los hiperplanos de máximo margen.
 [9] Corinna Cortés y Vapnik propusieron la actual encarnación estándar
 (margen blando) en 1993 y se publicaron en 1995.
 [1]
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Estos métodos están propiamente relacionados con problemas de clasificación
 y regresión.
 Dado un conjunto de ejemplos de entrenamiento (de muestras) podemos etiquetar
 las clases y entrenar una SVM para construir un modelo que prediga la clase
 de una nueva muestra.
 Intuitivamente, una SVM lineal es un modelo que representa a los puntos
 de muestra en el espacio, separando las clases a 2 espacios lo más amplios
 posibles mediante un hiperplano de separación definido como el vector entre
 los 2 puntos, de las 2 clases, mas cercanos al que se llama vector soporte.
 Cuando las nuevas muestras se ponen en correspondencia con dicho modelo,
 en función de los espacios a los que pertenezcan, pueden ser clasificadas
 a una o la otra clase.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Más formalmente, una SVM construye un hiperplano o conjunto de hiperplanos
 en un espacio de dimensionalidad muy alta (o incluso infinita) que puede
 ser utilizado en problemas de clasificación o regresión 
\begin_inset CommandInset citation
LatexCommand citep
key "AnIntroductiontoSupportVectorMachinesandotherkernel-basedlearningmethods"

\end_inset

.
 Una buena separación entre las clases permitirá un clasificación correcta.
 Una SVM busca un hiperplano que separe de forma óptima a los puntos de
 una clase de la de otra, que eventualmente han podido ser previamente proyectad
os a un espacio de dimensionalidad superior.
 En ese concepto de "separación óptima" es donde reside la característica
 fundamental de las SVM: este tipo de algoritmos buscan el hiperplano que
 tenga la máxima distancia (margen) con los puntos que estén más cerca de
 él mismo.
 Por eso también a veces se les conoce a las SVM como clasificadores de
 margen máximo.
 De esta forma, los puntos del vector que son etiquetados con una categoría
 estarán a un lado del hiperplano y los casos que se encuentren en la otra
 categoría estarán al otro lado.
 Los algoritmos SVM pertenecen a la familia de los clasificadores lineales.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Si entramos en una explicación matemática, dado un cojunto de datos de n
 puntos, 
\begin_inset Formula $(\overrightarrow{x_{i}},y_{i}),...,(\overrightarrow{x_{n}},y_{n})$
\end_inset

, donde 
\begin_inset Formula $y_{i}$
\end_inset

 es 1 o -1, cada uno indicando la clase a la cual el punto 
\begin_inset Formula $\overrightarrow{x_{i}}$
\end_inset

 pertenece.
 Cada 
\begin_inset Formula $\overrightarrow{x_{i}}$
\end_inset

 es un vector real p-dimensional.
 Se quiere encontrar el "hiperplano de máximo margen" que divide el grupo
 de puntos 
\begin_inset Formula $\overrightarrow{x_{i}}$
\end_inset

 para los cuales 
\begin_inset Formula $y_{i}=1$
\end_inset

 de los que 
\begin_inset Formula $y_{i}=-1$
\end_inset

 , el cual es definido por la distancia entre el hiperplano y el punto 
\begin_inset Formula $\overrightarrow{x_{i}}$
\end_inset

 más cercano de cada grupo.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Cualquier hiperplano puede ser escrito como el conjunto de datos de puntos
 
\begin_inset Formula $\overrightarrow{x_{i}}$
\end_inset

 que cumpla:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overrightarrow{w}*\overrightarrow{x}-b=0
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
donde 
\begin_inset Formula $\overrightarrow{w}$
\end_inset

 es el vector nomal (no necesariamente normalizado) a el hiperplano.
 El parámetro 
\begin_inset Formula $\frac{b}{||\overrightarrow{w}\text{||}}$
\end_inset

determina el desplazamiento del hiperplano desde el origen a lo largo del
 vector 
\begin_inset Formula $\overrightarrow{w}$
\end_inset

.
\end_layout

\begin_layout Subsection
Perceptrón multicapa
\end_layout

\begin_layout Standard
Un perceptron de múltiples capas (MLP 
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "MLP"
description "Multilayer perceptron"

\end_inset

) es un modelo de red neuronal artificial de retropropagación (control o
 modificación usando resultados anticipados) que asigna conjuntos de datos
 de entrada a un conjunto de salidas apropiadas.
 Un MLP consiste en múltiples capas de nodos en un gráfico dirigido, con
 cada capa completamente conectada al siguiente.
 Excepto para los nodos de entrada, cada nodo es una neurona (o elemento
 de procesamiento ) con una función de activación no lineal.
 MLP utiliza una técnica de aprendizaje supervisado llamada backpropagation
 para entrenar la red.
 
\begin_inset CommandInset citation
LatexCommand citep
key "PrinciplesofNeurodynamics:PerceptronsandtheTheoryofBrainMechanisms"

\end_inset


\begin_inset CommandInset citation
LatexCommand citep
key "LearninginternalRepresentationsbyErrorPropagation"

\end_inset

MLP es una modificación del perceptron lineal estándar y puede distinguir
 datos que no son linealmente separables
\begin_inset CommandInset citation
LatexCommand citep
key "Approximationbysuperpositionsofasigmoidalfunction"

\end_inset

.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Los perceptrones de múltiples capas que utilizan un algoritmo de retropropagació
n son el algoritmo estándar para cualquier proceso supervisado de reconocimiento
 de patrones de aprendizaje y el tema de la investigación en curso en neurocienc
ia computacional y procesamiento distribuido paralelo.
 Son útiles en la investigación en términos de su capacidad para resolver
 problemas estocásticamente, lo que a menudo permite obtener soluciones
 aproximadas para problemas extremadamente complejos.
\end_layout

\begin_layout Standard
MLPs son aproximadores de función universal como se muestra por el teorema
 de Cybenko, 
\begin_inset CommandInset citation
LatexCommand citep
key "Approximationbysuperpositionsofasigmoidalfunction"

\end_inset

 por lo que pueden ser utilizados para crear modelos matemáticos mediante
 análisis de regresión.
 Como la clasificación es un caso particular de regresión cuando la variable
 de respuesta es categórica , las MLP son también buenos algoritmos clasificador
es.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Los MLP eran una solución de aprendizaje de máquina popular en los años
 80, encontrando aplicaciones en diversos campos tales como reconocimiento
 de voz, reconocimiento de imagen, y software de traducción automática 
\begin_inset CommandInset citation
LatexCommand citep
key "Neuralnetworks.II.Whataretheyandwhyiseverybodysointerestedinthemnow?"

\end_inset

, pero desde la década de 1990 se enfrentó a la fuerte competencia de la
 mucho más simple SVM.
 Más recientemente, ha habido un renovado interés en las redes de retropropagaci
ón debido a los éxitos del aprendizaje profundo.
\end_layout

\begin_layout Subsection
Credal Decision Trees (CDT
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "CDT"
description "Credal Decision Trees"

\end_inset

) 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Credal-Decision-Trees"

\end_inset


\end_layout

\begin_layout Standard
El Credal Decision Trees es un árbol de decisión desarrollado por Abellán
 y Moral 
\begin_inset CommandInset citation
LatexCommand citep
key "Buildingclassificationtreesusingthetotaluncertaintycriterion"

\end_inset

, usando la teoría de probabilidades imprecisas presentada por Walley 
\begin_inset CommandInset citation
LatexCommand citep
key "Inferencesfrommultinomialdatalearningaboutabagofmarbles"

\end_inset

, conocido por Imprecise Dirichlet Model (IDM
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "IDM"
description "Imprecise Dirichlet Model"

\end_inset

).
 El proceso de selección de variables para este algoritmo se basa en probabilida
des imprecisas y medidas de incertidumbre en conjuntos credales, es decir
 cerrados y convexos de distribuciones de probabilidad.
 De esta manera, este algoritmo considera que el conjunto de entrenamiento
 no es confiable cuando se lleva a cabo el proceso de selección de variables.
 Este método obtiene buenos resultados experimentales, especialmente cuando
 se clasifican datos ruidodos 
\begin_inset CommandInset citation
LatexCommand citep
key "Baggingschemesonthepresenceofnoiseinclassification,Analysisandextensionofdecisiontreesbasedonimpreciseprobabilities:applicationonnoisydata"

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
key "Analysisandextensionofdecisiontreesbasedonimpreciseprobabilities:applicationonnoisydata"

\end_inset

.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
El criterio de división utilizado para construir CDTs se basa en probabilidades
 imprecisas y en la aplicación de medidas de incertidumbre en los conjuntos
 credales 
\begin_inset CommandInset citation
LatexCommand citep
key "Buildingclassificationtreesusingthetotaluncertaintycriterion"

\end_inset

.
 La base matemática de este prodecimiento puede describirse: sea una variable
 
\begin_inset Formula $Z$
\end_inset

 una variable con valores en 
\begin_inset Formula $\{z_{1,}...,z_{k}\}$
\end_inset

.
 Supongamos una distribuición de probabilidad 
\begin_inset Formula $p(zj),j=1,...,k$
\end_inset

 definida por cada valor de 
\begin_inset Formula $z_{j}$
\end_inset

del conjunto de datos.
\end_layout

\begin_layout Standard
IDM es usado para estimar los intervalos de probabilidad del conjunto de
 datos para cada valor de la variable 
\begin_inset Formula $Z$
\end_inset

 de la siguiente forma 
\begin_inset CommandInset citation
LatexCommand citep
key "Inferencesfrommultinomialdatalearningaboutabagofmarbles"

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p(z_{j})\epsilon\left[\frac{n_{z_{j}}}{N+s},\frac{n_{z_{j}}+s}{N+s}\right],j=1,...,k;
\]

\end_inset


\end_layout

\begin_layout Standard
con 
\begin_inset Formula $n_{z_{j}}$
\end_inset

 como la frecuencia de los valores 
\begin_inset Formula $Z=z_{j}$
\end_inset

en el conjunto de datos, 
\begin_inset Formula $N$
\end_inset

el tamaño de la muestra y 
\begin_inset Formula $s$
\end_inset

 hiperparámetro dado.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Este tipo de reperesentación ofrece un tipo específico de conjunto credal
 en la variable 
\begin_inset Formula $Z,K(Z)$
\end_inset

 definido como 
\begin_inset CommandInset citation
LatexCommand citep
key "UncertaintymeasuresonprobabilityintervalsfromImpreciseDirichletmodel"

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
K(Z)=\left\{ p|p(z_{j})\epsilon\left[\frac{n_{z_{j}}}{N+s},\frac{n_{z_{j}}+s}{N+s}\right],j=1,...,k\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard
En este tipo de conjuntos, pueden ser aplicadas medidas imprecisas.
 El procedimiento para construir CDTs utilza la función máxima de entropía
 en el cojunto de credales definido anteriormente.
 Esta función denominada como 
\begin_inset Formula $H*$
\end_inset

, se define como
\begin_inset Formula $H*(K(Z))=max\{H(p)|P\epsilon K(Z)\}$
\end_inset

 donde la función 
\begin_inset Formula $H$
\end_inset

 es la función de entropía de Shannon 
\begin_inset CommandInset citation
LatexCommand citep
key "UncertaintymeasuresonprobabilityintervalsfromImpreciseDirichletmodel"

\end_inset

.
 
\begin_inset Formula $H*$
\end_inset

 es una medida de incertidumbre total que es bien conocida para este tipo
 de cojuntos.
 El procedimiento para 
\begin_inset Formula $H*$
\end_inset

 en el IDM alcanza su coste mas bajo para 
\begin_inset Formula $s\leq1$
\end_inset

.
\end_layout

\end_body
\end_document
