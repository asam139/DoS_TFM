#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass ociamthesis-lyx
\begin_preamble
%Packages
\usepackage{listings}
\end_preamble
\options a4paper,titlepage
\use_default_options false
\maintain_unincluded_children false
\language spanish
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing onehalf
\use_hyperref false
\papersize a4paper
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type numerical
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 2
\paperpagestyle headings
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
Metodología
\begin_inset CommandInset label
LatexCommand label
name "chap:Metodología"

\end_inset


\end_layout

\begin_layout Standard
En esta sección se explicará detalladamente cada uno de los clasificadores
 objetos de estudio.
 Se comentará la procedencia de cada uno de ellos, ya sea estadística o
 de la IA, además se explicarán matemáticamente los fundamentos de cada
 uno de ellos.
\end_layout

\begin_layout Section
Modelos
\end_layout

\begin_layout Standard
En esta sección describiremos los diferentes tipos de modelos de clasificación
 que se utilizarán sobre la base de datos para su posterior comparación
 y validación.
 Se van a utilizar una serie de clasificadores estándares como son el C4.5,
 Naïve Bayes, LogR, SVM y Perceptrón multicapa.
 Y además se va a incorporar uno nuevo, el Credal Decision Trees (CDT),
 para ver como trabaja para la detección de ataques DDoS.
 
\end_layout

\begin_layout Subsection
Algoritmo C4.5
\end_layout

\begin_layout Standard
Es un algoritmo usado para genera un árbol de decisión desarrollado por
 Ross Quinlan 
\begin_inset CommandInset citation
LatexCommand citep
key "ProgramsForMachineLearning"

\end_inset

.
 Se trata de una extensión del algoritmo ID3 también presentado anteriormente
 por Quinlan.
 Aunque los arboles de decisión que este algoritmo genera tienen múltiples
 aplicaciones una de las más importantes es como clasificador.
 El J48 es una implementación de código abierto en WEKA en el lenguaje de
 programación Java.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Los algoritmos como el ID3 y el C4.5 que pertenecen a la familia Top Down
 Induction Trees (TDIDT
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "TDIDT"
description "Top Down Induction Trees"

\end_inset

), se construyen a partir del método de Hunt
\begin_inset CommandInset citation
LatexCommand citep
key "quinlan1986induction"

\end_inset

.
 El esqueleto de este método para construir un árbol de decisión a partir
 de un conjunto 
\begin_inset Formula $T$
\end_inset

 de datos de entrenamiento es muy simple.
 Sean las clases 
\begin_inset Formula ${C_{1},C_{2},C_{3},...,C_{k}}$
\end_inset

.
 Este algoritmo tiene unos pocos casos base.
\end_layout

\begin_layout Itemize
\begin_inset Formula $T$
\end_inset

 contiene uno o más casos, todos pertenecientes a una única clase 
\begin_inset Formula $C_{j}$
\end_inset

: el árbol de decisión para 
\begin_inset Formula $T$
\end_inset

 es una hoja identificando la clase 
\begin_inset Formula $C_{j}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
\begin_inset Formula $T$
\end_inset

 no contiene ningún caso: el árbol de decisión es una hoja, pero la clase
 asociada debe ser determinada por información que no pertenece a 
\begin_inset Formula $T.$
\end_inset

 Por ejemplo, una hoja puede escogerse de acuerdo a conocimientos de base
 del dominio, como ser la clase mayoritaria.
\end_layout

\begin_layout Itemize
\begin_inset Formula $T$
\end_inset

 contiene casos pertenecientes a varias clases: en este caso, la idea es
 refinar 
\begin_inset Formula $T$
\end_inset

 en subconjuntos de casos que tiendan, o parezcan tender hacia una colección
 de casos pertenecientes a una única clase.
\end_layout

\begin_layout Standard
C4.5 construye árboles de decisión desde un grupo de datos de entrenamiento
 de la misma forma en que lo hace ID3, usando el concepto de entropía de
 información 
\begin_inset CommandInset citation
LatexCommand citep
key "ProgramsForMachineLearning"

\end_inset

.
 La entropía mide la incertidumbre de una fuente de información, o también
 la cantidad de información que contienen los datos usados 
\begin_inset CommandInset citation
LatexCommand citep
key "Teoradelainformacincodificacinylenguajes"

\end_inset

.
 La entropía suponiendo que un evento (variable aleatoria) tiene un grado
 de indeterminación inicial igual a 
\begin_inset Formula $k$
\end_inset

 (es decir existen 
\begin_inset Formula $k$
\end_inset

 estados posibles) y supongamos todos los estados son equiprobables, entonces
 la probabilidad de que se dé una de esas combinaciones será 
\begin_inset Formula $p=\frac{1}{k}$
\end_inset

.
 Luego podemos representar la expresión 
\begin_inset Formula ${\displaystyle c_{i}}$
\end_inset

 como:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
{\displaystyle c_{i}=\log_{2}(k)=\log_{2}[1/(1/k)]=\log_{2}(1/p)=\underbrace{\log_{2}(1)}_{=0}-\log_{2}(p)=-\log_{2}(p)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Si ahora cada uno de los 
\begin_inset Formula $k$
\end_inset

 estados tiene una probabilidad 
\begin_inset Formula ${\displaystyle p_{i}}$
\end_inset

, entonces la entropía vendrá dada por la suma ponderada de la cantidad
 de información 
\begin_inset CommandInset citation
LatexCommand citep
key "Teoradelainformacincodificacinylenguajes"

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
H=-p_{1}\log_{2}(p_{1})-p_{2}\log_{2}(p_{2})-....-p_{k}\log_{2}(p_{k})=-\sum_{i=1}^{k}p_{i}\log_{2}(p_{i})
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Por lo tanto, la entropía de un mensaje 
\begin_inset Formula ${\displaystyle X}$
\end_inset

, denotado por 
\begin_inset Formula ${\displaystyle H(X)}$
\end_inset

, es el valor medio ponderado de la cantidad de información de los diversos
 estados del mensaje 
\begin_inset CommandInset citation
LatexCommand citep
key "Teoradelainformacincodificacinylenguajes"

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
{\displaystyle H(X)=-\sum_{i}p(x_{i})\log_{2}p(x_{i})}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
que representa una medida de la incertidumbre media acerca de una variable
 aleatoria y por tanto de la cantidad de información.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
En ID3 y posteriormente en C4.5, Quinlan hace una representación intuitiva
 de los árboles de clasificación, donde cada nodo se puede ver como una
 fuente de información de memoria nula, cuyo alfabeto tiene tantos símbolos
 como clases.
 Cada nodo al tener almacenado un subcojunto de casos del conjunto de datos,
 guarda una información media sobre las clases de esos casos.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
En ID3, el mejor atributo X para ramificar el árbol dado el conjunto de
 datos D, es aquel qeu maximiza la ganancia de información.
 Para calcular la ganancia de cada atributo se realizan los siguientes pasos:
\end_layout

\begin_layout Itemize
Se calcula la entropía de la clase C antes de ramificar, o lo que viene
 a ser lo mismo, la entropía de C para el conjunto D:
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\[
H_{D}(C)=-\sum_{i=1}^{k}p_{i}\log_{2}(p_{i})
\]

\end_inset

 donde 
\begin_inset Formula $p_{i}$
\end_inset

 representa la probabilidad (calculada por frecuencias relativas) de la
 clase 
\begin_inset Formula $i$
\end_inset

 en D y 
\begin_inset Formula $n$
\end_inset

es el número de clases.
\end_layout

\begin_layout Itemize
Se calcula el valor medio de la entropía en el nivel siguiente, generado
 por el atributo 
\begin_inset Formula $X$
\end_inset

:
\begin_inset Formula 
\[
H_{D}(C|X)=\sum_{i=1}^{k}p_{D}(X=x_{j})H_{D_{j}}(C)
\]

\end_inset

 donde 
\begin_inset Formula $\{x_{1,}x_{2,}...,x_{k}\}$
\end_inset

 son los valores del atributo, 
\begin_inset Formula $X,p_{d}(X=x_{j})$
\end_inset

representa la probabilidad condicionada de que 
\begin_inset Formula $X=x_{j}$
\end_inset

en 
\begin_inset Formula $D$
\end_inset

 y 
\begin_inset Formula $D_{j}$
\end_inset

 es el subconjunto de 
\begin_inset Formula $D,D_{j}\subset D$
\end_inset

, tal que sus elementos tienen el valor de 
\begin_inset Formula $X=x_{j}$
\end_inset

.
\end_layout

\begin_layout Itemize
Se denomina ganancia de la información de la variable X, a la diferencia
 de entropía de la entropía de C para el conjunto D menos la entropía media
 del siguiente nivel si ramificásemos por 
\begin_inset Formula $X$
\end_inset

:
\begin_inset Formula 
\[
Ganancia_{D}(C|X)=H_{D}(C)-H_{D}(C|X)
\]

\end_inset


\end_layout

\begin_layout Standard
En ID3, se escoge la variable con mayor ganancia de información.
 En C4.5 además se calcula la Información de ruptura, y la razón de ganancia.
 Para un conjunto de datos 
\begin_inset Formula $D$
\end_inset

 y un atributo 
\begin_inset Formula $X$
\end_inset

, se define la razón de ganancia de la siguiente forma:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
RazónGanancia(X|C)=\frac{Ganancia_{D}(C|X)}{InfoRuptura_{D}(X)}
\]

\end_inset


\end_layout

\begin_layout Standard
donde,
\begin_inset Formula 
\[
InfoRuptura_{D}(X)=-\sum_{j=1}^{k}p_{d}(X=x_{j})\log_{2}(p_{D}(X=x_{j}))
\]

\end_inset


\end_layout

\begin_layout Standard
La prueba basada en el criterio de máxima ganancia, tiende a escoger aquellas
 variables con mayor número de valores.
 Esto es debido a que cuantas más particiones se hagan debido a los valores
 de la variable, la entropía de un nuevo nodo será, normalmente menor.
 En C4.5 se utilza la razón de ganancia para paliar esta tendencia.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
El algoritmo C4.5 también se diferencia de ID3 porque admite datos continuos,
 dividiendo el dominio de la variable continua en subintervalos por el valor
 que tiene una mayor ganancia de información.
 Además C4.5 admite valores desconocidos en los ejemplos, modificando los
 criterios de ganancia e información de ruptura.
\end_layout

\begin_layout Subsection
Naïve Bayes
\end_layout

\begin_layout Standard
El clasificador Naïve Bayes es un simple clasificador probabilístico basado
 en la aplicación del teorema de Bayes con suposiciones de independencia
 fuertes (ingenuas) entre las características 
\begin_inset CommandInset citation
LatexCommand citep
key "ModelosBayesianosparalaclasificacinsupervisada.Aplicacionesalanlisisdedatosdeexpresingrfica"

\end_inset

.
 Naïve Bayes ha sido muy estudiado desde los años 50.
 Durante todos estos años sus aplicaciones han sido múltiples, una de ellas
 es como un método popular para la categorización de textos, es decir si
 son de política, deportes, anuncios o correo basura.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Naïve Bayes un algoritmo basado en un principio común: el valor de una caracterí
stica particular es independiente del valor de cualquier otra característica
 dada la variable de clase.
 Por ejemplo, una fruta puede ser considerada como una manzana si es roja,
 redonda y de unos 10 cm de diámetro.
 Un clasificador Naïve Bayes considera que cada una de estas características
 contribuye independientemente a la probabilidad de que este fruto sea una
 manzana, independientemente de las posibles correlaciones entre el color,
 la redondez y las características del diámetro.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Para algunos tipos de modelos de probabilidad, los clasificadores ingenuos
 de Bayes pueden ser entrenados de manera muy eficiente en un entorno de
 aprendizaje supervisado .
 En muchas aplicaciones prácticas, la estimación de parámetros para modelos
 ingenuos de Bayes utiliza el método de máxima verosimilitud; En otras palabras,
 se puede trabajar con él sin aceptar la probabilidad bayesiana o usar cualquier
 método bayesiano.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
A pesar de su diseño ingenuo y supuestos aparentemente simplificados, los
 clasificadores Naïve Bayes han funcionado bastante bien en muchas situaciones
 complejas del mundo real.
 En 2004, un análisis 
\begin_inset CommandInset citation
LatexCommand citep
key "TheOptimallyofNaiveBayes"

\end_inset

 del problema de la clasificación bayesiana mostró que existen razones teóricas
 sólidas para la aparentemente improbable eficacia de los clasificadores
 Naïve Bayes.
 Sin embargo, una comparación completa con otros algoritmos de clasificación
 en 2006 
\begin_inset CommandInset citation
LatexCommand citep
key "Unacomparacinempricadealgoritmosdeaprendizajesupervisado"

\end_inset

 mostró que la clasificación de Naïve Bayes es superada por otros enfoques,
 tales como los booted trees o los random forests.
\end_layout

\begin_layout Standard
Una ventaja de Naïve Bayes es que sólo requiere un pequeño número de datos
 de entrenamiento para estimar los parámetros necesarios para la clasificación.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Abstractamente, Naïve Bayes es un modelo de probabilidad condicional: dado
 un conjunto de datos a ser clasificados, representado por un vector 
\begin_inset Formula $x=(x_{1,}x_{2,}...,x_{n}$
\end_inset

 representando algunas características o atributos, 
\begin_inset Formula $n$
\end_inset

 (variables independientes), asigna a esta instancia las probabilidades
 
\begin_inset Formula $p(C_{k}|x_{1},...,x_{n})$
\end_inset

 para cada uno de los 
\begin_inset Formula $K$
\end_inset

 posibles resultados o clases 
\begin_inset Formula $C_{k}$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
key "PatternRecognition:AnAlgorithmicApproach."

\end_inset

.
 
\end_layout

\begin_layout Standard
El problema con la formulación anterior es que si el número de características
 
\begin_inset Formula $n$
\end_inset

 es grande o si una característica puede asumir un gran número de valores,
 entonces basar dicho modelo en tablas de probabilidad es inviable.
 Por lo tanto, se puede reformular el modelo para que sea más manejable.
 Usando el teorema de Bayes , la probabilidad condicional se puede descomponer
 como
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
p(C_{k}|x)=\frac{p(C_{k})p(x|C_{k})}{p(x)})\longrightarrow posterior=\frac{anterior*probabilidad}{evidencia}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
En la práctica, sólo hay interés en el numerador de esa fracción, porque
 el denominador no depende de 
\begin_inset Formula $C$
\end_inset

.
 Y los valores de las características 
\begin_inset Formula $F_{i}$
\end_inset

, de modo que el denominador es efectivamente constante.
 El numerador es equivalente a una probabilidad compuesta: 
\begin_inset Formula $p(C_{k},x_{1},...,x_{n})$
\end_inset

, que puede ser reescrita como sigue, aplicando repetidamente la definición
 de probabilidad condicional:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{multline}
p(C,F1,...,F_{n}=p(C)p(F_{1},...,F_{n}|C)=p(C)p(F_{1}|C)p(F_{2},...F_{n}|C,F_{1})=\\
p(C)p(F_{1}|C)p(F_{2}|C,F_{1})p(F_{3},...F_{n}|C,F_{1},F_{2})
\end{multline}

\end_inset


\end_layout

\begin_layout Standard
y así sucesivamente.
 Ahora es cuando el principio común de 
\begin_inset Quotes eld
\end_inset

naïve
\begin_inset Quotes erd
\end_inset

 de independencia condicional se aplica: se asume que cada 
\begin_inset Formula $F_{i}$
\end_inset

es independiente de cualquier otra 
\begin_inset Formula $F_{j}$
\end_inset

 para 
\begin_inset Formula $j\text{≠}i$
\end_inset

.
 Esto significa que 
\begin_inset Formula $p(F_{i}|C,F_{j})=p(F_{i}|C)$
\end_inset

, y por lo tanto que la probabilidad compuesta puede expresarse como:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
p(C,F_{1},...,F_{n})=p(C)p(F_{1}|C)p(F_{2}|C)p(F_{3}|C)...=p(C)\prod_{i=1}^{n}p(F_{i}|C)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Esto significa que haciendo estas asunciones, la distribución condicional
 sobre la variable clasificaroria C puede expresarse de la siguiente manera
 
\begin_inset CommandInset citation
LatexCommand citep
key "PatternRecognition:AnAlgorithmicApproach."

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
p(C|F_{1},...,F_{n})=\frac{1}{Z}p(C)\prod_{i=1}^{n}p(F_{i}|C)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
donde 
\begin_inset Formula $Z$
\end_inset

 es un factor que depende sólo de 
\begin_inset Formula $F_{1},...,F_{n}$
\end_inset

, es decir, constante si los valores de 
\begin_inset Formula $F_{i}$
\end_inset

 son conocidos.
\begin_inset Newline newline
\end_inset

Hasta ahora se ha desarrollado el modelo de probabilidad de Bayes.
 Pero el clasificador combina este modelo junto con la regla de decisión.
 Una regla común es escoger la hípotesís mas probable; esto se conoce como
 el máximo a posteriori o regla de decisión MAP (
\shape italic
Maximum a Posteriori Probability
\shape default
)
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "MAP"
description "Maximum a Posteriori Probability"

\end_inset

.
 Por lo tanto el clasificador de Bayes es la función que asigna una etiqueta
 de clase 
\begin_inset Formula $\hat{y}=C_{k}$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
key "PatternRecognition:AnAlgorithmicApproach."

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{y}=\underset{k\epsilon\{1,...,K\}}{argmax}p(C_{k})\prod_{i=1}^{n}p(F_{i}|C)
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
LogR: Logictic Regression
\end_layout

\begin_layout Standard
La regresión logística o LogR
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "LogR"
description "Logictic Regression"

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
key "ModelosEstadsticos:TeorayPrctica"

\end_inset

 es un tipo de análisis de regresión utilizado para predecir el resultado
 de una variable que se puede categorizar, es decir, una variable que puede
 adoptar un número limitado de categorías en función de variables, característic
as o atributos.
 Es útil para modelar la probabilidad de un evento como función de otros
 factores.
 El análisis LogR se enmarca en el conjunto de Ge (GLM por sus siglas en
 inglés) que usa como función de enlace la función logit.
 Las probabilidades que describen el posible resultado de un único ensayo
 se modelan, como una función de variables explicativas, utilizando una
 función logística.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
La función logic para un número p queda determinada por la siguiente expresión:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
{\displaystyle logit(p)=\log\left(\frac{p}{1-p}\right)=\log(p)-\log(1-p)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
La variable solo puede tener dos valores, es decir sólo dos valores 
\begin_inset Quotes eld
\end_inset

0
\begin_inset Quotes erd
\end_inset

 y 
\begin_inset Quotes eld
\end_inset

1
\begin_inset Quotes erd
\end_inset

, se denomina regresión lógistica binaria, la cual se explicará.
 En cambio si puede tener más ya se de habla de regresión lógistica multinomial.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
La regresión logística fue desarrollada por el estadístico David Cox en
 1958 
\begin_inset CommandInset citation
LatexCommand citep
key "Estimationoftheprobabilityofaneventasafunctionofseveralindependentvariables"

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
key "Theregressionanalysisofbinarysequenceswithdiscussion"

\end_inset

.
 El modelo logístico binario se utiliza para estimar la probabilidad de
 una respuesta binaria basada en una o más variables predictor (o independientes
) (características).
 Permite decir que la presencia de un factor de riesgo aumenta la probabilidad
 de un determinado resultado por un porcentaje específico.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
La regresión logística se utiliza en varios campos, incluyendo el aprendizaje
 automático, la mayoría de los campos médicos y las ciencias sociales.
 Por ejemplo, la regresión logística puede usarse para predecir si un paciente
 tiene una enfermedad dada (por ejemplo, diabetes, enfermedad coronaria),
 basándose en las características observadas del paciente (edad, sexo, índice
 de masa corporal 
\begin_inset CommandInset citation
LatexCommand citep
key "ValidationofMPIandOIAIIintwodifferentgroupsofpatientswithsecondaryperitonitis"

\end_inset

 , resultados de varios análisis de sangre , etc.).
 Ser utilizados en ingeniería, especialmente para predecir la probabilidad
 de fallo de un proceso, sistema o producto dado.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Una explicación de la regresión logística puede comenzar con una explicación
 de la función logística estándar.
 La función logística es útil porque puede tomar cualquier entrada real
 
\begin_inset Formula $t$
\end_inset

, 
\begin_inset Formula $(t\epsilon R)$
\end_inset

 , mientras que la salida siempre toma valores entre cero y uno 
\begin_inset CommandInset citation
LatexCommand citep
key "AppliedLogisticRegression"

\end_inset

 y por lo tanto es interpretable como una probabilidad.
 La función logística 
\begin_inset Formula $\sigma(t)$
\end_inset

 se define como:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\sigma(t)=\frac{e^{t}}{e^{t}+1}=\frac{1}{1+e^{-t}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Suponiendo que 
\begin_inset Formula $t$
\end_inset

 es una función lineal de una sola variable 
\begin_inset Formula $x$
\end_inset

, entonces se puede expresar 
\begin_inset Formula $t$
\end_inset

 como:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
t=\beta_{0}+\beta_{1}x
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Y la función lógistica ahora se puede escribir como:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
F(x)=\frac{1}{1+e^{-(\beta_{0}+\beta_{1}x)}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $F(x)$
\end_inset

 se interpreta como la probabilidad de que la variable dependiente sea igual
 a 
\begin_inset Quotes eld
\end_inset

éxito
\begin_inset Quotes erd
\end_inset

 o 
\begin_inset Quotes eld
\end_inset

caso
\begin_inset Quotes erd
\end_inset

, más que fallo o sin éxito.
 Y haciendo uso de F(x) se define la inversa de la función logística, logit,
 como:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
g(F(x))=ln\left(\frac{F(x)}{1-F(x)}\right)=\beta_{0}+\beta_{1}x\longrightarrow\frac{F(x)}{1-F(x)}=e^{\beta_{0}+\beta_{1}x}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Las probabilidades de que la variable dependiente sea igual a un caso (dada
 alguna combinación lineal 
\begin_inset Formula $x$
\end_inset

 de las variables predictoras) es equivalente a la función exponencial de
 la expresión de regresión lineal.
 Esto ilustra cómo el logit sirve como una función de enlace entre la probabilid
ad y la expresión de regresión lineal.
 Dado que el logit oscila entre infinito negativo y positivo, proporciona
 un criterio adecuado sobre el cual conducir la regresión lineal y el logit
 se convierte fácilmente de nuevo en las probabilidades 
\begin_inset CommandInset citation
LatexCommand citep
key "AppliedLogisticRegression"

\end_inset

.
 
\end_layout

\begin_layout Subsection
SVM: Support Vector Machine
\end_layout

\begin_layout Standard
Las máquinas de soporte vectorial 
\begin_inset CommandInset citation
LatexCommand citep
key "AnIntroductiontoSupportVectorMachinesandotherkernel-basedlearningmethods"

\end_inset

, máquinas de vectores de soporte o máquinas de vector soporte (Support
 Vector Machines, SVMs) son un conjunto de algoritmos pensados para resolver
 problemas de clasificación binaria, actualmente se utilizan para resolver
 otros tipos de probleamas (regresión, agrupamiento, multiclasificación)
 desarrollados por Vladimir N.
 Vapnik y Alexey Ya.
 Chervonenkis en 1963.
 En 1992, Bernhard E.
 Boser , Isabelle M.
 Guyon y Vladimir N.
 Vapnik sugirieron una manera de crear clasificadores no lineales aplicando
 el truco del núcleo a los hiperplanos de máximo margen.
 Corinna Cortés y Vapnik 
\begin_inset CommandInset citation
LatexCommand citep
key "cortes1995support"

\end_inset

 propusieron la actual encarnación estándar (margen blando) en 1993 y se
 publicaron en 1995.
\begin_inset CommandInset citation
LatexCommand citep
key "AnIntroductiontoSupportVectorMachinesandotherkernel-basedlearningmethods"

\end_inset

 También han sido utilizados en más campos con éxito, tales como visión
 artificial, reconocimiento de caracteres, categorización de texto e hipertexto,
 clasificación de proteínas, análisis de series temporales.
 De hecho, desde su introducción, han ido ganando un merecido reconocimiento
 gracias a sus sólidos fundamentos teóricos.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Dentro de la tarea de clasificación, las SVMs pertenecen a la categoría
 de los clasificadores lineales 
\begin_inset CommandInset citation
LatexCommand citep
key "boser1992training"

\end_inset

, puesto que inducen separadores lineales o hiperplanos, ya sea en el espacio
 original de los ejemplos de entrada, si éstos son separabales o cuasi-separable
s (ruido), o en un espacio transformado (espacio de características), si
 los ejemplos no son separables linealmente en el espacio original.
 Como se verá más adelante, la búsqueda del hiperplano de separación en
 estos espacios transformados, normalmente de muy alta dimensión, se hará
 de forma implícita utilizando las denominadas funciones kernel.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Mientras la mayoría de los método de aprendizaje se centran en minimzar
 los errores cometidos por el modelo generado a partir de los ejemplos de
 entrenamiento (error empírico), el sesgo inductivo asociado a las SVMs
 radica en la minimización del denominado riesgo estructural.
 La idea es seleccionar un hiperplano de separación qeu equidista de los
 ejemplos más cercano de cada clase para, de esta forma, conseguir lo que
 se denomina un margen máximo a cada lado del hiperplano.
 Además, a la hora de definir el hiperplano, sólo se consideran los ejemplos
 de entrenamiento de cada clase qeu caen justo en la frontera de dichos
 márgenes.
 Estos ejemplos reciben el nombre de vectores soporte.
 Desde un punto de vista práctico, el hiperplano separador de margen máximo
 ha demostrado tener una buena capacidad de generalización, evitando en
 gran medida el problema del sobreajuste a los ejemplos de entrenamiento.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Si entramos en una explicación matemática 
\begin_inset CommandInset citation
LatexCommand citep
key "boser1992training"

\end_inset

, dado un conjunto separable de ejemplos, 
\begin_inset Formula $S=(\overrightarrow{x_{i}},y_{i}),...,(\overrightarrow{x_{n}},y_{n})$
\end_inset

, donde 
\begin_inset Formula $x_{i}\epsilon R^{d}$
\end_inset

e 
\begin_inset Formula $y_{i}\epsilon\{+1,-1\}$
\end_inset

, se puede definir un hiperplano de separación (ver figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Hiperplanos-de-separación"

\end_inset

) como una función lineal que es capaz de separa dicho cojunto sin error:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
D(x)=(w_{1}x_{1}+...+w_{d}x_{d})+b=<w,x>+b
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
donde 
\begin_inset Formula $w$
\end_inset

 y 
\begin_inset Formula $b$
\end_inset

 son coeficientes reales.
 El hiperplano de separación cumplirá las siguientes restricciones 
\begin_inset CommandInset citation
LatexCommand citep
key "boser1992training"

\end_inset

 para todo 
\begin_inset Formula $x_{i}$
\end_inset

 del conjunto de ejemplos:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
y_{i}D(x_{i})\geq0,\;i=1,...,n
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Tal y como se puede deducir de la figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Hiperplanos-de-separación"

\end_inset

, el hiperplano que permite separar los ejemplos no es único, es decir,
 existen infinitos hiperplanos separables, representados por todos aquellols
 hiperplanos que son capaces de cumplir las restricciones impuestas.
 Surge entonces la pregunta de cual de ellos es el óptimo.
 Para ello, primero se define el concepto de margen de un hiperplano de
 separación, denotado por 
\begin_inset Formula $\tau$
\end_inset

, como la mínima distancia entre dicho hiperplano y el ejemplo más cercano
 de cualquiera de las dos clases.
 A partir de esta definición, un hiperplano de separación se denominará
 óptimo si su margen es de tamaño máximo.
\end_layout

\begin_layout Standard
\align block
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename /Users/Sau/Documents/MasterTeleco/TFM/Memoria/figures/hiperplanos1.png
	width 75text%
	rotateOrigin center

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Hiperplanos de separación en un espacio bidimensional de un cojunto de ejemplos
 separables en dos clases.
 (a) ejemplo de hiperplano de separación (b) otros ejemplos de hiperplanos
 de separacion de entre los infinitos posibles.
\begin_inset CommandInset label
LatexCommand label
name "fig:Hiperplanos-de-separación"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Por geometría, se sabe que la distancia entre un hiperplano de separación
 
\begin_inset Formula $D(x)$
\end_inset

 y un ejemplo 
\begin_inset Formula $\text{x'}$
\end_inset

viene dada por:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\frac{\left|D(x')\right|}{\left\Vert w\right\Vert }
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
siendo 
\begin_inset Formula $w$
\end_inset

el vector que, junto con el parámetor 
\begin_inset Formula $b$
\end_inset

, define el hiperplano 
\begin_inset Formula $D(x)$
\end_inset

 y que, además, tiene la propiedad de ser perpendicular al hiperplano considerad
o.
 Haciendo uso de esta expresión y las restricciones, todos los ejemplos
 de entrenamiento cumplirán que 
\begin_inset CommandInset citation
LatexCommand citep
key "boser1992training"

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\frac{y_{i}D(x')}{\left\Vert w\right\Vert }\geq\tau,\;i=1,...,n
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
De la expresión anterior, se deduce que el hiperplano óptimo es equivalente
 a encontrar el valor de w que maximiza el margen.
 Sin embargo, existen infinitas soluciones que difieren en la escala de
 
\begin_inset Formula $w$
\end_inset

.
 Para limitar las soluciones la escala de producto de 
\begin_inset Formula $\tau$
\end_inset

 y la norma de 
\begin_inset Formula $w$
\end_inset

 se fija, de forma arbitraria, a la unidad, es decir
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\tau\left\Vert w\right\Vert =1
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Llegando a la conclusión final 
\begin_inset CommandInset citation
LatexCommand citep
key "boser1992training"

\end_inset

 de que aumentar el margen es equivalente a disminuir la norma de 
\begin_inset Formula $w$
\end_inset

.
 Por tanto, de acuerdo a su definición, un hiperplano de separación óptimo
 será aquel que posee un margen máximo y, por tanto un valor mínimo de 
\begin_inset Formula $\left\Vert w\right\Vert $
\end_inset

y, además, está sujeto a la restricción, junto con límitación impuesta.
\end_layout

\begin_layout Subsection
Perceptrón multicapa
\end_layout

\begin_layout Standard
Un perceptron de múltiples capas (MLP 
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "MLP"
description "Multilayer perceptron"

\end_inset


\begin_inset CommandInset citation
LatexCommand citep
key "PrinciplesofNeurodynamics:PerceptronsandtheTheoryofBrainMechanisms"

\end_inset


\begin_inset CommandInset citation
LatexCommand citep
key "LearninginternalRepresentationsbyErrorPropagation"

\end_inset

) es un modelo de red neuronal artificial de retropropagación (control o
 modificación usando resultados anticipados) que asigna conjuntos de datos
 de entrada a un conjunto de salidas apropiadas.
 Un MLP consiste en múltiples capas de nodos en un gráfico dirigido, con
 cada capa completamente conectada al siguiente.
 Excepto para los nodos de entrada, cada nodo es una neurona (o elemento
 de procesamiento ) con una función de activación no lineal.
 MLP utiliza una técnica de aprendizaje supervisado llamada backpropagation
 para entrenar la red.
 MLP es una modificación del perceptron lineal estándar y puede distinguir
 datos que no son linealmente separables
\begin_inset CommandInset citation
LatexCommand citep
key "Approximationbysuperpositionsofasigmoidalfunction"

\end_inset

.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Los perceptrones de multicapa que utilizan un algoritmo de retropropagación
 del error son el algoritmo estándar para cualquier proceso supervisado
 de reconocimiento de patrones de aprendizaje y el tema de la investigación
 en curso en neurociencia computacional y procesamiento distribuido paralelo.
 Son útiles en la investigación en términos de su capacidad para resolver
 problemas estocásticamente, lo que a menudo permite obtener soluciones
 aproximadas para problemas extremadamente complejos.
\end_layout

\begin_layout Standard
MLPs son aproximadores de función universal como se muestra por el teorema
 de Cybenko 
\begin_inset CommandInset citation
LatexCommand citep
key "Approximationbysuperpositionsofasigmoidalfunction"

\end_inset

, por lo que pueden ser utilizados para crear modelos matemáticos mediante
 análisis de regresión.
 Como la clasificación es un caso particular de regresión cuando la variable
 de respuesta es categórica, los MLP son también buenos algoritmos clasificadore
s.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Los MLP eran una solución de aprendizaje de máquina popular en los años
 80, encontrando aplicaciones en diversos campos tales como reconocimiento
 de voz, reconocimiento de imagen, y software de traducción automática 
\begin_inset CommandInset citation
LatexCommand citep
key "Neuralnetworks.II.Whataretheyandwhyiseverybodysointerestedinthemnow?"

\end_inset

, pero desde la década de 1990 se enfrentó a la fuerte competencia de la
 mucho más simple SVM.
 Más recientemente, ha habido un renovado interés en las redes de retropropagaci
ón debido a los éxitos del aprendizaje profundo 
\begin_inset CommandInset citation
LatexCommand citep
key "lecun2015deep"

\end_inset

.
\end_layout

\begin_layout Subsection
Credal Decision Trees (CDT
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "CDT"
description "Credal Decision Trees"

\end_inset

) 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Credal-Decision-Trees"

\end_inset


\end_layout

\begin_layout Standard
El Credal Decision Trees es un árbol de decisión desarrollado por Abellán
 y Moral 
\begin_inset CommandInset citation
LatexCommand citep
key "Buildingclassificationtreesusingthetotaluncertaintycriterion"

\end_inset

, usando la teoría de probabilidades imprecisas presentada por Walley 
\begin_inset CommandInset citation
LatexCommand citep
key "Inferencesfrommultinomialdatalearningaboutabagofmarbles"

\end_inset

, conocido por Imprecise Dirichlet Model (IDM
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "IDM"
description "Imprecise Dirichlet Model"

\end_inset

).
 El proceso de selección de variables para este algoritmo se basa en probabilida
des imprecisas y medidas de incertidumbre en conjuntos credales, es decir
 cerrados y convexos de distribuciones de probabilidad.
 De esta manera, este algoritmo considera que el conjunto de entrenamiento
 no es confiable cuando se lleva a cabo el proceso de selección de variables.
 Este método obtiene buenos resultados experimentales, especialmente cuando
 se clasifican datos ruidodos 
\begin_inset CommandInset citation
LatexCommand citep
key "Baggingschemesonthepresenceofnoiseinclassification,Analysisandextensionofdecisiontreesbasedonimpreciseprobabilities:applicationonnoisydata"

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
key "Analysisandextensionofdecisiontreesbasedonimpreciseprobabilities:applicationonnoisydata"

\end_inset

.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
El criterio de selección de variable para ramificar utilizado para construir
 CDTs se basa en probabilidades imprecisas y en la aplicación de medidas
 de incertidumbre en los conjuntos credales 
\begin_inset CommandInset citation
LatexCommand citep
key "Buildingclassificationtreesusingthetotaluncertaintycriterion"

\end_inset

.
 La base matemática de este procedimiento puede describirse: sea una variable
 
\begin_inset Formula $Z$
\end_inset

 una variable con valores en 
\begin_inset Formula $\{z_{1,}...,z_{k}\}$
\end_inset

.
 Supongamos una distribuición de probabilidad 
\begin_inset Formula $p(zj),j=1,...,k$
\end_inset

 definida por cada valor de 
\begin_inset Formula $z_{j}$
\end_inset

 del conjunto de datos.
\end_layout

\begin_layout Standard
IDM es usado para estimar los intervalos de probabilidad del conjunto de
 datos para cada valor de la variable 
\begin_inset Formula $Z$
\end_inset

 de la siguiente forma 
\begin_inset CommandInset citation
LatexCommand citep
key "Inferencesfrommultinomialdatalearningaboutabagofmarbles"

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
p(z_{j})\epsilon\left[\frac{n_{z_{j}}}{N+s},\frac{n_{z_{j}}+s}{N+s}\right],j=1,...,k;
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
con 
\begin_inset Formula $n_{z_{j}}$
\end_inset

 como la frecuencia de los valores 
\begin_inset Formula $Z=z_{j}$
\end_inset

 en el conjunto de datos, 
\begin_inset Formula $N$
\end_inset

el tamaño de la muestra y 
\begin_inset Formula $s$
\end_inset

 hiperparámetro dado.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Este tipo de reperesentación ofrece un tipo específico de conjunto credal
 en la variable 
\begin_inset Formula $Z,K(Z)$
\end_inset

 definido como 
\begin_inset CommandInset citation
LatexCommand citep
key "UncertaintymeasuresonprobabilityintervalsfromImpreciseDirichletmodel"

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
K(Z)=\left\{ p|p(z_{j})\epsilon\left[\frac{n_{z_{j}}}{N+s},\frac{n_{z_{j}}+s}{N+s}\right],j=1,...,k\right\} 
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
En este tipo de conjuntos, pueden ser aplicadas medidas imprecisas.
 El procedimiento para construir CDTs utilza la función máxima de entropía
 en el cojunto de credales definido anteriormente.
 Esta función denominada como 
\begin_inset Formula $H*$
\end_inset

, se define como: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
H*(K(Z))=max\{H(p)|P\epsilon K(Z)\}
\end{equation}

\end_inset

 donde la función 
\begin_inset Formula $H$
\end_inset

 es la función de entropía de Shannon 
\begin_inset CommandInset citation
LatexCommand citep
key "UncertaintymeasuresonprobabilityintervalsfromImpreciseDirichletmodel"

\end_inset

.
 
\begin_inset Formula $H*$
\end_inset

 es una medida de incertidumbre total que es bien conocida para este tipo
 de conjuntos.
 El procedimiento para 
\begin_inset Formula $H*$
\end_inset

 en el IDM (Walley’s Imprecise Dirichlet Model
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "IDM"
description "Walley’s Imprecise Dirichlet Model"

\end_inset

) alcanza su coste mas bajo para 
\begin_inset Formula $s\leq1$
\end_inset

.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
En este modelo la Ganancia de Información Imprecisa es:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
IID(C,X)=H*(K(Z))-{\displaystyle \sum_{i}}P(X=x_{i})H*(K(Z|X=x_{i}))
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
en el cual 
\begin_inset Formula $H*$
\end_inset

 determina la máxima entropía de los conjuntos credales 
\begin_inset Formula $K(Z)$
\end_inset

 y 
\begin_inset Formula $K(Z|X=x_{i}))$
\end_inset

, respectivamente.
 Estos ultimos conjuntos se obtienen gracias al Modelo de Dirichlet Impreciso
 tanto para la variable clase 
\begin_inset Formula $Z$
\end_inset

 como para la misma condicionada al valor del atributo i-esimo 
\begin_inset Formula $(Z|X=x_{i})$
\end_inset

.
 Ademas, 
\begin_inset Formula $P(X=x_{i})\nabla i=1,...,n$
\end_inset

 denota la distribucion de probabilidad precisa correspondiente.
\begin_inset Newline newline
\end_inset

s
\end_layout

\begin_layout Standard
El algoritmo para construir un CDT es muy similar al del ID3.
 Se tiene en cuenta que cada nodo 
\begin_inset Formula $N_{0}$
\end_inset

 en un árbol de decisión, produce una partición del conjunto de datos, que
 nombramos para simplificar como 
\begin_inset Formula $D$
\end_inset

.
 Cada nodo 
\begin_inset Formula $N_{0}$
\end_inset

 tiene una lista asociada 
\begin_inset Formula $L$
\end_inset

 de etiquetas de características (que no están en el camino desde el nodo
 raíz a 
\begin_inset Formula $N_{0}$
\end_inset

 ).
 El procedimiento para construir un CDT se explica en el algoritmo 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:Procedimiento-para-construir"

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
key "abellan2017comparative"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[mathescape=true]   
\end_layout

\begin_layout Plain Layout

Procedimiento ConstruirCDT(No, L)	
\end_layout

\begin_layout Plain Layout

	
\end_layout

\begin_layout Plain Layout

	1.
 Si L = 0, then Salir 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	2.
 Sea D la partición asociada con el nodo No
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	3.
 Computar el valor 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

		$
\backslash
alpha=max
\backslash
:x_{j}
\backslash
epsilon L
\backslash
left
\backslash
{ IIG^{D}(C,X_{j})
\backslash
right
\backslash
}$
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	4.
 Si $
\backslash
alpha
\backslash
leq0$ entonces Salir
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	5.
 Sino
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

		6.Sea la variable $X_l$ para cual el máximo de $
\backslash
alpha$ es alcanzado 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	7.
 Eliminar $
\backslash
alpha
\backslash
leq0$ de $L$
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	8.
 Asignar $X_l$ al nodo $N_o$
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	9.
 Por cada posible valor $x_l$ de $X_l$
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

		10.
 Anadir un nodo $N_{o_{l}}$
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

		11.
 Hacer un hijo $N_{o_{l}}$ de $N_o$
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

		12.
 LLamar ConstruirCDT($N_{o_{l}}$, $L$)
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Procedimiento para construir un CDT
\begin_inset CommandInset label
LatexCommand label
name "alg:Procedimiento-para-construir"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_body
\end_document
